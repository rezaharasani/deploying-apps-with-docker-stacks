# Deploying Swarm services  

Services let us specify most of the familiar container options such as name, port mappings, attaching to networks, and images. But they add important cloud-native features, including `desired state` and `reconciliation`. For example, swarm services let us define the desired state for an application and let the swarm take care of deploying it and managing it.  

Let’s look at a quick example. Assume you have an app with a web front-end. You have an image for the web server and testing has shown that you need 5 instances to handle normal daily traffic. You translate this requirement into a single `service` declaring the image to use, and that the service should always have 5 running replicas. You issue that to the swarm as your desired state and the swarm takes care of ensuring there are always 5 instances of the web server running.  

You can create services in one of two ways:
1. Imperatively on the command line with `docker service create`
2. Declaratively with a stack file  

```
swarm-manager-01:~# docker service create --name web-fe -p 8080:8080 --replicas 5 \
  nigelpoulton/ddd-book:web0.1
l7cppyy983athxca8p9oij9vq
overall progress: 5 out of 5 tasks 
1/5: running   [==================================================>] 
2/5: running   [==================================================>] 
3/5: running   [==================================================>] 
4/5: running   [==================================================>] 
5/5: running   [==================================================>] 
verify: Service l7cppyy983athxca8p9oij9vq converged
```

**Terminology**: Services deploy containers, and we often call these containers replicas. For example, a service that deploys three replicas will deploy three identical containers.  

The command was sent to a manager node and the *leader manager* instantiated 5 replicas across the swarm. Managers on this swarm aren’t allowed to run application containers, meaning all 5 replicas are deployed to worker nodes. Each worker that received a work task pulled the image and started a replica listening on port *8080*. The swarm leader also ensured a copy of the desired state was stored on the cluster and replicated to every manager.  

But this isn’t the end. All *services* are constantly monitored by the swarm — the swarm runs a background `reconciliation loop` that constantly compares the `observed state` of the service with the `desired state`. If the two states match, the world is a happy place and no further action is needed. If they don’t match, swarm takes actions to bring *observed state* into line with *desired state*.  

As an example, if a worker hosting one of the 5 replicas fails, the observed state of the service will drop from 5 replicas to 4 and will no longer match the desired state of 5. As a result, the swarm will start a new replica to bring the observed state back in line with desired state. We call this `reconciliation` or `self-healing` and it’s a key tenet of cloud-native applications.  


### Viewing and inspecting services  
You can use the `docker service ls` command to see a list of all services running on a swarm.  
```
swarm-manager-01:~# docker service ls
ID             NAME       MODE         REPLICAS   IMAGE               PORTS
l7cppyy983at   web-fe     replicated   5/5        nigel...web0.1      *:8080->8080/tcp
```  

The output shows a single service and some basic config and state info. Among other things, we can see the name of the service and that 5 out of the 5 desired replicas are running. If you run this command soon after deploying the service it might not show all replicas as running. This is usually while the workers pull the image.  

You can use the `docker service ps` command to see a list of service replicas and the state of each.
```
swarm-manager-01:~# docker service ps web-fe 
ID             NAME       IMAGE            NODE              DESIRED STATE   CURRENT STATE 
hbo1341t3gaj   web-fe.1   nigel...web0.1   swarm-worker-03   Running         Running 11 minutes ago
01pb55cy036a   web-fe.2   nigel...web0.1   swarm-worker-03   Running         Running 11 minutes ago
o0629yh0cq62   web-fe.3   nigel...web0.1   swarm-worker-01   Running         Running 11 minutes ago
2vqievoc1pc2   web-fe.4   nigel...web0.1   swarm-worker-02   Running         Running 11 minutes ago
kfnsrl005ki9   web-fe.5   nigel...web0.1   swarm-worker-01   Running         Running 11 minutes ago
```  

The format of the command is `docker service ps <service-name or service-id>`. The output displays each replica on its own line, shows the node it’s running on, and shows desired state and the current observed state.  

For detailed information about a service, use the `docker service inspect` command.
```
ID:		l7cppyy983athxca8p9oij9vq
Name:		web-fe
Service Mode:	Replicated
 Replicas:	5
Placement:
UpdateConfig:
 Parallelism:	1
 On failure:	pause
 Monitoring Period: 5s
 Max failure ratio: 0
 Update order:      stop-first
RollbackConfig:
 Parallelism:	1
 On failure:	pause
 Monitoring Period: 5s
 Max failure ratio: 0
 Rollback order:    stop-first
ContainerSpec:
 Image:		nigelpoulton/ddd-book:web0.1@sha256:6b8e0d14b583...2ef8297fcaaa2a350be
 Init:		false
Resources:
Endpoint Mode:	vip
Ports:
 PublishedPort = 8080
  Protocol = tcp
  TargetPort = 8080
  PublishMode = ingress 
```

The example uses the --pretty flag to limit the output to the most interesting items printed in an easy-to-read format. Leaving off the --pretty flag will give you more info.  


### Replicated vs global services  
The default replication mode of a service is `replicated`. This deploys a desired number of replicas and distributes them as evenly as possible across the cluster. The other mode is `global`. This runs a single replica on every node in the swarm.  

To deploy a *global service* you need to pass the `--mode global` flag to the `docker service create` command. Global services do not accept the `--replicas` flag as they always run one replica per available node. However, they do respect a node’s *availability*.  

For example, if you’ve drained manager nodes so they don’t run application containers, global services will not schedule replicas to them.  


### Scaling a service
Another powerful feature of *services* is the ability to easily scale them up and down. Fortunately, we can easily scale the service up with the `docker service scale` command.  
```
swarm-manager-01:~# docker service scale web-fe=10
web-fe scaled to 10
overall progress: 10 out of 10 tasks 
1/10: running   [==================================================>] 
2/10: running   [==================================================>] 
3/10: running   [==================================================>] 
4/10: running   [==================================================>] 
5/10: running   [==================================================>] 
6/10: running   [==================================================>] 
7/10: running   [==================================================>] 
8/10: running   [==================================================>] 
9/10: running   [==================================================>] 
10/10: running   [==================================================>] 
verify: Service web-fe converged
```  

This command will scale the number of service replicas from 5 to 10. In the bachground it’s updating the service’s desired state from 5 to 10. Run another `docker service ls` command to verify the operation was successful.  
```
swarm-manager-01:~# docker service ls
ID             NAME      MODE         REPLICAS   IMAGE            PORTS
l7cppyy983at   web-fe    replicated   10/10      nigel...web0.1   *:8080->8080/tcp
```

Running a `docker service ps` command will show that the service replicas are balanced across all nodes in the swarm evenly.
```
swarm-manager-01:~# docker service ps web-fe
ID             NAME        IMAGE            NODE              DESIRED STATE   CURRENT STATE 
hbo1341t3gaj   web-fe.1    nigel...web0.1   swarm-worker-03   Running         Running 23 minutes ago
01pb55cy036a   web-fe.2    nigel...web0.1   swarm-worker-03   Running         Running 23 minutes ago
o0629yh0cq62   web-fe.3    nigel...web0.1   swarm-worker-01   Running         Running 23 minutes ago
2vqievoc1pc2   web-fe.4    nigel...web0.1   swarm-worker-02   Running         Running 23 minutes ago
kfnsrl005ki9   web-fe.5    nigel...web0.1   swarm-worker-01   Running         Running 23 minutes ago
oxpbvxlqzxut   web-fe.6    nigel...web0.1   swarm-worker-02   Running         Running 2 minutes ago
3orh9tfkwvmx   web-fe.7    nigel...web0.1   swarm-worker-03   Running         Running 2 minutes ago
kevjynowx9bt   web-fe.8    nigel...web0.1   swarm-worker-01   Running         Running 2 minutes ago
z4wdmyasp785   web-fe.9    nigel...web0.1   swarm-worker-02   Running         Running 2 minutes ago
uh3rwkyqwiuq   web-fe.10   nigel...web0.1   swarm-worker-02   Running         Running 2 minutes ago
```

Behind the scenes, Swarm runs a scheduling algorithm called **spread** that attempts to balance replicas as evenly as possible across available nodes.  

Run another `docker service scale` command to bring the number down from 10 to 5.
```
swarm-manager-01:~# docker service scale web-fe=5
web-fe scaled to 5
overall progress: 5 out of 5 tasks 
1/5: running   [==================================================>] 
2/5: running   [==================================================>] 
3/5: running   [==================================================>] 
4/5: running   [==================================================>] 
5/5: running   [==================================================>] 
verify: Service web-fe converged
```

### Removing services
Run the following `docker service rm` command to delete the web-fe service.  
```
swarm-manager-01:~# docker service rm web-fe
web-fe
```
Confirm it’s gone with the `docker service ls` command.
```
swarm-manager-01:~# docker service ls
ID             NAME      MODE         REPLICAS   IMAGE               PORTS
```  

**Note**: Be careful using this command as it deletes all service replicas without asking for confirmation.


### Rolling updates

To see this, we’re going to deploy a new service. But before we do that, we’re going to create a new overlay network for the service. This isn’t necessary, but I want you to see how it is done and how to attach the service to it.  
```
swarm-manager-01:~# docker network create -d overlay uber-net
hpadwii6inib4x7a9pbllchzb
```

Run a `docker network ls` to verify that the network created properly and is visible on the Docker host.  
```
swarm-manager-01:~# docker network ls
NETWORK ID     NAME              DRIVER    SCOPE
hpadwii6inib   uber-net          overlay   swarm
<Snip>
```

The `uber-net` network was successfully created with the swarm scope and is currently *only visible on manager nodes* in the swarm. It will be dynamically extended to worker nodes when they run workloads configured on the network.  

An *overlay network* is a layer 2 network that can span all swarm nodes. All containers on the same overlay network will be able to communicate, even if they’re deployed to different nodes. This works even if all swarm nodes are on different underlying networks.  

Figure shows four swarm nodes connected to two different underlay networks connected by a layer 3 router. The overlay network spans all 4 nodes and creates a single flat layer 2 network that abstracts all the underlying networking.  

![image](https://github.com/arianariamehr/docker-swarm-stack-sample/assets/130653489/625052d7-f672-4a3f-a9a3-4b1644fe5ede)


Let’s create a new service and attach it to the network.  
```
swarm-manager-01:~# docker service create --name uber-svc --network uber-net \
  -p 80:80 --replicas 12 nigelpoulton/tu-demo:v1
lytcgpj71ajz23ry80j0vvjry
overall progress: 12 out of 12 tasks 
1/12: running   [==================================================>] 
2/12: running   [==================================================>] 
3/12: running   [==================================================>] 
4/12: running   [==================================================>] 
5/12: running   [==================================================>] 
6/12: running   [==================================================>] 
7/12: running   [==================================================>] 
8/12: running   [==================================================>] 
9/12: running   [==================================================>] 
10/12: running   [==================================================>] 
11/12: running   [==================================================>] 
12/12: running   [==================================================>] 
verify: Service lytcgpj71ajz23ry80j0vvjry converged
```  

Run a `docker service ls` and a `docker service ps` command to verify the state of the new service.
```
swarm-manager-01:~# docker service ls
ID             NAME       MODE         REPLICAS   IMAGE                                       PORTS
lytcgpj71ajz   uber-svc   replicated   12/12      nigelpoulton/tu-demo:v1                     *:80->80/tcp

swarm-manager-01:~# docker service ps uber-svc 
ID             NAME          IMAGE                 NODE               DESIRED STATE   CURRENT STATE
rtfmwcobfbsi   uber-svc.1    nigelpoulton/...:v1   swarm-manager-02   Running         Running 32 minutes ago
ixygxtj2hv8q   uber-svc.2    nigelpoulton/...:v1   swarm-manager-03   Running         Running 32 minutes ago
zjhwd7ttx3st   uber-svc.3    nigelpoulton/...:v1   swarm-worker-01    Running         Running 32 minutes ago
yqvj14hrpdnz   uber-svc.4    nigelpoulton/...:v1   swarm-worker-02    Running         Running 32 minutes ago
3n5xsdz57q7b   uber-svc.5    nigelpoulton/...:v1   swarm-worker-03    Running         Running 32 minutes ago
ufbqw0wsuq00   uber-svc.6    nigelpoulton/...:v1   swarm-manager-03   Running         Running 32 minutes ago
vyccs4whiero   uber-svc.7    nigelpoulton/...:v1   swarm-manager-01   Running         Running 32 minutes ago
sbqf6r22innk   uber-svc.8    nigelpoulton/...:v1   swarm-manager-02   Running         Running 32 minutes ago
hqmc9fso9tt1   uber-svc.9    nigelpoulton/...:v1   swarm-worker-03    Running         Running 32 minutes ago
i02d5kgpkb97   uber-svc.10   nigelpoulton/...:v1   swarm-worker-01    Running         Running 32 minutes ago
is8l6u8nhb7e   uber-svc.11   nigelpoulton/...:v1   swarm-worker-02    Running         Running 32 minutes ago
cfp5vk6rxb81   uber-svc.12   nigelpoulton/...:v1   swarm-manager-01   Running         Running 32 minutes ago
```  

This mode of publishing a port on every node in the swarm — even nodes not running service replicas — is
called `ingress mode` and is the default. The alternative mode is `host mode` which only publishes the service on swarm nodes running replicas. Publishing a service in host mode requires the long-form syntax and looks like the following:
```
swarm-manager-01:~# docker service create --name uber-svc
  --network uber-net \
  --publish published=80,target=80,mode=host \
  --replicas 12 \
  nigelpoulton/tu-demo:v1
```

Open a web browser and point it to the IP address of any of the nodes in the swarm on port 80 to see the service running.  

![image](https://github.com/arianariamehr/docker-swarm-stack-sample/assets/130653489/a69122c9-fc92-4213-9d4e-985d7bfef9cd)

As you can see, it’s a simple voting application that will register votes for either “football” or “soccer”. You’ll be able to reach the web service from any node because the -p 80:80 flag creates an *ingress mode* mapping on every swarm node. This is true even on nodes that are not running a replica for the service — every node gets a mapping and can therefore *redirect* your request to a node that is running the service.  

Let’s also assume that you’ve been tasked with pushing the updated image to the swarm in a staged manner — 2 replicas at a time with a 20 second delay between each. You can use the following `docker service update` command to accomplish this.  

```
swarm-manager-01:~# docker service update --image nigelpoulton/tu-demo:v2 \
  --update-parallelism 2 \
  --update-delay 20s \
  uber-svc 

overall progress: 8 out of 12 tasks 
1/12: running   
2/12: running   
3/12: running   
4/12: running   
5/12: running   
6/12: running   
7/12: running   
8/12: running   
9/12: starting   
10/12: ready   
11/12: 
12/12: 
```

If you run a `docker service ps uber-svc` while the update is in progress, some of the replicas will be at v2 while some will still be at v1. If you give the operation enough time to complete (4 minutes), all replicas will eventually reach the new desired state of using the v2 image.  

```
swarm-manager-01:~# docker service ps uber-svc  
ID             NAME              IMAGE        NODE               DESIRED STATE   CURRENT STATE       
uxfkbho1cq5h   uber-svc.1        nigel...v2   swarm-manager-02   Running         Running 5 minutes ago
rtfmwcobfbsi    \_ uber-svc.1    nigel...v1   swarm-manager-02   Shutdown        Shutdown 5 minutes ago
xdy3n4hx5wmv   uber-svc.2        nigel...v2   swarm-manager-03   Running         Running 5 minutes ago
ixygxtj2hv8q    \_ uber-svc.2    nigel...v1   swarm-manager-03   Shutdown        Shutdown 5 minutes ago
s5xxc8qa94jn   uber-svc.3        nigel...v2   swarm-worker-03    Running         Running 6 minutes ago
zjhwd7ttx3st    \_ uber-svc.3    nigel...v1   swarm-worker-01    Shutdown        Shutdown 6 minutes ago
tjx77h82a8tm   uber-svc.4        nigel...v2   swarm-worker-02    Running         Running 5 minutes ago
yqvj14hrpdnz    \_ uber-svc.4    nigel...v1   swarm-worker-02    Shutdown        Shutdown 5 minutes ago
obp177rdqbm8   uber-svc.5        nigel...v2   swarm-worker-01    Running         Running 6 minutes ago
3n5xsdz57q7b    \_ uber-svc.5    nigel...v1   swarm-worker-03    Shutdown        Shutdown 6 minutes ago
mzppaug1c6vv   uber-svc.6        nigel...v2   swarm-manager-03   Running         Running 4 minutes ago
ufbqw0wsuq00    \_ uber-svc.6    nigel...v1   swarm-manager-03   Shutdown        Shutdown 4 minutes ago
jsxb49nhcwt4   uber-svc.7        nigel...v2   swarm-manager-01   Running         Running 4 minutes ago
vyccs4whiero    \_ uber-svc.7    nigel...v1   swarm-manager-01   Shutdown        Shutdown 4 minutes ago
hao1l7jjnq8q   uber-svc.8        nigel...v2   swarm-manager-02   Running         Running 5 minutes ago
sbqf6r22innk    \_ uber-svc.8    nigel...v1   swarm-manager-02   Shutdown        Shutdown 5 minutes ago
uq150th8vj73   uber-svc.9        nigel...v2   swarm-worker-03    Running         Running 6 minutes ago
hqmc9fso9tt1    \_ uber-svc.9    nigel...v1   swarm-worker-03    Shutdown        Shutdown 6 minutes ago
5brqvas0q9yt   uber-svc.10       nigel...v2   swarm-worker-01    Running         Running 5 minutes ago
i02d5kgpkb97    \_ uber-svc.10   nigel...v1   swarm-worker-01    Shutdown        Shutdown 5 minutes ago
2mhtorafr1ki   uber-svc.11       nigel...v2   swarm-worker-02    Running         Running 6 minutes ago
is8l6u8nhb7e    \_ uber-svc.11   nigel...v1   swarm-worker-02    Shutdown        Shutdown 6 minutes ago
d63kqtsxvi9n   uber-svc.12       nigel...v2   swarm-manager-01   Running         Running 5 minutes ago
cfp5vk6rxb81    \_ uber-svc.12   nigel...v1   swarm-manager-01   Shutdown        Shutdown 5 minutes ago
```  

You can witness the update happening in real-time by opening a web browser to any node in the swarm and
hitting refresh several times. Some of the requests will be serviced by replicas running the old version and some will be serviced by replicas running the new version. After enough time, all requests will be serviced by replicas running the updated version of the service.  

If you run a `docker inspect --pretty` command against the service, you’ll see the update parallelism and update delay settings are now part of the service definition. This means future updates will automatically use these seings unless you override them as part of the `docker service update` command.  
```
swarm-manager-01:~# docker service inspect --pretty uber-svc 

ID:		lytcgpj71ajz23ry80j0vvjry
Name:		uber-svc
Service Mode:	Replicated
 Replicas:	12
UpdateStatus:
 State:		completed
 Started:	18 minutes ago
 Completed:	15 minutes ago
 Message:	update completed
Placement:
UpdateConfig:
 Parallelism:	2
 Delay:		20s
 On failure:	pause
 Monitoring Period: 5s
 Max failure ratio: 0
 Update order:      stop-first
RollbackConfig:
 Parallelism:	1
 On failure:	pause
 Monitoring Period: 5s
 Max failure ratio: 0
 Rollback order:    stop-first
ContainerSpec:
 Image:		nigelpoulton/tu-demo:v2@sha256:90487574e0c73ffa3c1c0d38727d9990f9347a5a84918d4f78c7942220de754c
 Init:		false
Resources:
Networks: uber-net 
Endpoint Mode:	vip
Ports:
 PublishedPort = 80
  Protocol = tcp
  TargetPort = 80
  PublishMode = ingress 

```

You should also note a couple of things about the service’s network config. All nodes in the swarm that are running a replica for the service will have the *uber-net* overlay network that we created earlier. We can verify this by running `docker network ls` on any node running a replica.  

You should also note the Networks portion of the `docker inspect` output. This shows the uber-net network as well as the swarm-wide 80:80 port mapping.  

Congratulations. You’ve just pushed a rolling update to a live containerized application.  
