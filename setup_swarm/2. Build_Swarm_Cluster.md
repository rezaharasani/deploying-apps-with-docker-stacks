# Build a secure Swarm cluster

In this section, we’ll build a secure swarm cluster with three *manager nodes* and three
*worker nodes*:

<p align="center">
  <img src="https://github.com/rezaharasani/deploying-apps-with-docker-stacks/assets/73277136/de59ecad-d76a-47c6-9b51-c4e7dea34459" width="600"/>
</p>

If you think you hit networking issues, be sure the following ports are open between all swarm nodes:
  - *2377/tcp*: for secure client-to-swarm communication  
  - *7946/tcp* and *udp*: for control plane gossip  
  - *4789/udp*: for VXLAN-based overlay networks  

## Initializing a new swarm  
The process of building a swarm is called *initializing a swarm*, and the high-level process is this: Initialize the first manager node > Join additional manager nodes > Join worker nodes > Done.  

Docker nodes that are not part of a swarm are said to be in **single-engine mode**. Once they’re added to a swarm they’re automatically switted into **swarm mode**.  

Running `docker swarm init` on a Docker host in single-engine mode will switch that node into *swarm mode*, create a new swarm, and make the node the first *manager* of the swarm. Additional nodes can then be joined to the swarm as workers and managers.  

**Note**: If the datetime of your host did not set, you must set correct timezone before starting following steps (Guide: https://orcacore.com/set-up-time-synchronization-debian-12-bookworm/)  

The following steps will initialize a new swarm:  

1. Log on to swarm-manager-01 and initialize a new swarm.  
    ```
    swarm-manager-01:~# docker swarm init \
        --advertise-addr 172.16.188.11:2377 \
        --listen-addr 172.16.188.11:2377
    Swarm initialized: current node (d0psdx1wo3jt5vk7lgrmrlksv) is now a manager.
    <Snip>
    ```

    The command can be broken down as follows:  

      -  `docker swarm init`: This tells Docker to initialize a new swarm and make this node the first manager. It also puts the node into *swarm mode*.  
      - `--advertise-addr`: This is the swarm API endpoint that will be advertised to other managers and workers. It will usually be one of the node’s IP addresses but can be an external load-balancer address. It’s an optional flag unless you need to specify a load-balancer or specific IP on a node with multiple IPs.  
      - `--listen-addr`: This is the IP address that the node will accept swarm traffic on. If you don’t set it, it defaults to the same value as `--advertise-addr`. If `--advertise-addr` is a load-balancer, you **must** use `--listen-addr` to specify a local IP or interface for swarm traffic.  

    **Note**: For production environments I recommend you be specific and always use both flags. It’s not so important for lab environments like ours.  

    The default port that Swarm mode operates on is **2377**. This is customizable, but it’s convention to use *2377/tcp* for secured (HTTPS) client-to-swarm connections.  

2. List the nodes in the swarm.  
    ```
    swarm-manager-01:~# docker node ls
    ID              HOSTNAME           STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
    dps...rlksv *   swarm-manager-01   Ready     Active         Leader           26.0.0
    
    ```

3. From swarm-manager-01 run the `docker swarm join-token` command to extract the commands and tokens required to add new workers and managers to the swarm.  
    ```
    swarm-manager-01:~# docker swarm join-token worker 
    To add a worker to this swarm, run the following command:
    
        docker swarm join --token  SWMTKN-1-0uahebax...c87tu8dx2c 172.16.188.11:2377

    
    swarm-manager-01:~# docker swarm join-token manager 
    To add a manager to this swarm, run the following command:
    
        docker swarm join --token SWMTKN-1-0uahebax...ue4hv6ps3p 172.16.188.11:2377
    ```  

    **Note**: Notice that the commands to join workers and managers are identical apart from the join tokens (SWMTKN...). This means that whether a node joins as a worker or a manager depends entirely on which token you use when joining it. **You should keep your join tokens in a safe place as they’re the only thing required to join a node to a swarm!**  


4. Log on to swarm-worker-01 and join it to the swarm using the `docker swarm join` command with the worker join token.  
    ```
    swarm-worker-01:~# docker swarm join \
        --token SWMTKN-1-0uahebax...c87tu8dx2c 172.16.188.11:2377 \
        --advertise-addr 172.16.188.21:2377 \
        --listen-addr 172.16.188.21:2377
    
    This node joined a swarm as a worker.
    ```  

    The `--advertise-addr`, and `--listen-addr` flags are optional. I’ve added them as I consider it best practice to be as specific as possible when it comes to network configuration in production environments. You probably don’t need them just for a lab.  

5. Repeat the previous step on swarm-worker-02 and swarm-worker-03 so that they join the swarm as workers. If you’re specifying the *--advertise-addr* and *--listen-addr* flags, make sure you use swarm-worker-02 and swarm-worker-03’s respective IP addresses.  

6. Log on to swarm-manager-02 and join it to the swarm as a manager using the `docker swarm join` command with the manager join token.  
    ```
    swarm-manager-02:~# docker swarm join \
        --token SWMTKN-1-0uahebax...ue4hv6ps3p 172.16.188.11:2377 \
        --advertise-addr 172.16.188.12:2377 \
        --listen-addr 172.16.188.12:2377
    
    This node joined a swarm as a manager.
    ```

7. Repeat the previous step on swarm-manager-03, remembering to use swarm-manager-03’s IP address for the *--advertise-addr* and *--listen-addr* flags.

8. List the nodes in the swarm by running `docker node ls` from any of the manager nodes.  
    ```
    swarm-manager-01:~# docker node ls
    ID                HOSTNAME           STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
    0g4rl...babl8     swarm-manager-01   Ready     Active         Leader           26.0.0
    d21ly...9qzkx *   swarm-manager-02   Ready     Active         Reachable        26.0.0
    2xlti...l0nyp     swarm-manager-03   Ready     Active         Reachable        26.0.0
    8yv0b...wmr67     swarm-worker-01    Ready     Active                          26.0.0
    9mzwf...e4m4n     swarm-worker-02    Ready     Active                          26.0.0
    e62gf...l5wt6     swarm-worker-03    Ready     Active                          26.0.0
    ```

Congratulations. You’ve just created a 6-node swarm with 3 managers and 3 workers. As part of the process, the Docker Engine on each node was automatically put into *swarm mode* and the swarm was automatically secured with TLS.  

**Note**: If you look in the *MANAGER STATUS* column you’ll see the three manager nodes are showing as either **Reachable** or **Leader**. Nodes with nothing in the *MANAGER STATUS* column are *workers*. Also the asterisk (*) after the ID tells you which node you’re executing commands from.  

Now that you have a swarm up and running, let’s take a look at manager high availability (HA).


## Swarm manager high availability (HA)  

Swarm *managers* have native support for high availability (HA). This means one or more can fail, and the survivors will keep the swarm running.  

Technically speaking, swarm implements **active/passive multi-manager** HA. This means only one manager is *active* at any given moment. This active manager is called the **leader**, and the leader is the only one that will ever issue updates to the swarm. So, it’s only ever the leader that changes the config, or issues tasks to workers. If a follower manager (*passive*) receives commands for the swarm, it proxies them across to the leader.  

<p align="center">
  <img src="https://github.com/rezaharasani/deploying-apps-with-docker-stacks/assets/73277136/d97bca92-ae41-4a16-8c28-7a08f4e67358" width="800"/>
</p>

*Leaders* and *followers* is *Raft* terminology. This is because swarm uses an implementation of the **Raft consensus algorithm** to maintain a consistent cluster state across multiple highly-available managers.  

On the topic of HA, the following two best practices apply:  
  1. Deploy an odd number of managers  
  2. Don’t deploy too many managers (3 or 5 is recommended)  
  3. Spread managers across availability zones  

Having an odd number of managers reduces the chances of **split-brain conditions**. For example, if we had 4 managers and the network partitioned, we could be left with two managers on each side of the partition. This is known as a split brain — each side knows there used to be 4 but can now only see 2. But crucially, neither side has any way of knowing if the other two are still alive and whether it holds a majority (quorum). Apps on a swarm cluster continue to operate during split-brain conditions, however, we’re not able to alter the configuration or add and manage application workloads.  

However, if we have 3 or 5 managers and the same network partition occurs, it is impossible to have an equal number of managers on both sides of the partition. This means that one knows it has a majority (**quorum**) and full cluster management services remain available.  

<p align="center">
  <img src="https://github.com/arianariamehr/docker-swarm-stack-sample/assets/130653489/6ac02471-efe3-4e3e-a948-de18485aec5a" width="800"/>
</p>

It’s a best practice to have either 3 or 5 managers for HA. 7 might work, but it’s generally accepted that 3 or 5 is optimal.  

A final word of caution regarding manager HA. While it’s a good practice to spread your managers across availability zones, you need to make sure the networks connecting them are reliable, as network partitions can be difficult to troubleshoot and resolve.  


## Built-in Swarm security  
Swarm clusters have a ton of built-in security that’s configured out-of-the-box with sensible defaults — CA settings, join tokens, mutual TLS, encrypted cluster store, encrypted networks, cryptographic node ID’s and more.  


## Locking a Swarm  
Despite all of this built-in native security, restarting an older manager or restoring an old backup has the potential to compromise the cluster. Old managers re-joining a swarm automatically decrypt and gain access to the Raft log time-series database — this can pose security concerns. Restoring old bachups can also wipe the current swarm configuration.  

To prevent situations like these, Docker allows you to lock a swarm with the Autolock feature. This forces restarted managers to present the cluster unlock key before being admitted back into the cluster.  

It’s possible to apply a lock directly to a new swarm by passing the `--autolock` flag to the `docker swarm init` command. However, we’ve already built a swarm, so we’ll lock our existing swarm with the `docker swarm update` command.  

Run the following command from a swarm manager.  
```
swarm-manager-01:~# docker swarm update --autolock=true
Swarm updated.
To unlock a swarm manager after it restarts, run the `docker swarm unlock`
command and provide the following key:

    SWMKEY-1-LyDlJJBVRwcKfBcdn6Ldg0o9AAOg9ODkhQErtCquqo8

Please remember to store this key in a password manager, since without it you
will not be able to restart the manager.
```  

Be sure to keep the unlock key in a secure place. You can always check your current swarm unlock key with the `docker swarm unlock-key` command.  

Restart one of your manager nodes to see if it automatically re-joins the cluster. You may need to prepend the command with *sudo*.  
```
swarm-manager-01:~#  service docker restart
``` 

Try and list the nodes in the swarm.  
```
swarm-manager-01:~# docker node ls
Error response from daemon: Swarm is encrypted and needs to be unlocked before it can be
used. Please use "docker swarm unlock" to unlock it.
```  

Although the Docker service has restarted on the manager, it has not been allowed to re-join the swarm. You can prove this even further by running the `docker node ls` command on another manager node. The restarted manager will show as `down` and `unreachable`.  

Run the `docker swarm unlock` command to unlock the swarm for the restarted manager. You’ll need to run this command on the restarted manager, and you’ll need to provide the unlock key.  
```
swarm-manager-01:~# docker swarm unlock
Please enter unlock key:  <enter your key>
```

The node will be allowed to re-join the swarm and will show as `ready` and `reachable` if you run another `docker node ls`.  

**Note**: Locking your swarm and protecting the unlock key is recommended for production environments.  

