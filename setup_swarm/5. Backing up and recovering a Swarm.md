# Backing up and recovering a Swarm
Backing up a swarm will backup the *control plane objects* required to recover the swarm in the event of a catastrophic failure of corruption. Recovering a swarm from a backup is an extremely rare scenario. However, business critical environments should always be prepared for worst-case scenarios. 
  
Swarm configuration and state is stored in `/var/lib/docker/swarm` on every manager node. The configuration includes; Raft log keys, overlay networks, Secrets, Configs, Services, and more. A swarm backup is a copy of all the files in this directory.  

As the contents of the directory are replicated to all managers, it’s possible to perform backups from multiple managers. However, as you have to stop the Docker daemon as part of the procedure, it’s a good idea to perform the backup from a non-leader manager. This is because stopping Docker on the leader will initiate a leader election. You should also perform the backup at a quiet time for the business, as stopping managers increases the risk of the swarm losing quorum if another manager fails during the backup.  

Create the following network before starting the backup. We’ll check for this in a later step after performing the recovery.  
```
swarm-manager-02:~# docker network create -d overlay unimatrix01
```  

The procedure we’re about to follow is a high-risk procedure and for demonstration purposes only. You’ll need to tweak it for your production environment. You may also have to prefix commands with sudo.  

1. Stop Docker on a non-leader manager.  
   If you have any containers or service replicas running on the node, this action may stop them. However, if you’ve been following along, your manager nodes won’t be running any application containers. If you locked your swarm, be sure to have a copy of the swarm unlock key.  
   ```
   swarm-manager-02:~# service docker stop
   ```
   
2. Backup the Swarm config.
   This example uses the Linux tar utility to perform the file copy that will be the backup. Feel free to use a different tool.  
   ```
   swarm-manager-02:~# tar -czvf swarm.bkp /var/lib/docker/swarm/
   tar: Removing leading `/' from member names
   /var/lib/docker/swarm/
   /var/lib/docker/swarm/docker-state.json
   /var/lib/docker/swarm/state.json
   /var/lib/docker/swarm/worker/
   /var/lib/docker/swarm/worker/tasks.db
   /var/lib/docker/swarm/raft/
   /var/lib/docker/swarm/raft/snap-v3-encrypted/
   /var/lib/docker/swarm/raft/snap-v3-encrypted/0000000000000004-0000000000000054.snap
   /var/lib/docker/swarm/raft/wal-v3-encrypted/
   /var/lib/docker/swarm/raft/wal-v3-encrypted/0000000000000000-0000000000000000.wal
   /var/lib/docker/swarm/certificates/
   /var/lib/docker/swarm/certificates/swarm-node.key
   /var/lib/docker/swarm/certificates/swarm-node.crt
   /var/lib/docker/swarm/certificates/swarm-root-ca.crt
   ```
   
3. Verify the backup file exists.
   ```
   swarm-manager-02:~# ls -l
   -rw-r--r-- 1 root root 1700500 Apr 11 18:31 swarm.bkp
   ```
   In the real world, you should store and rotate this backup in line with any corporate backup retention policies. At this point, the swarm is backed up and you can restart Docker on the node.  
   
4. Restart Docker.  
   ```
   swarm-manager-02:~# service docker restart
   
   swarm-manager-02:~# service docker status
   ● docker.service - Docker Application Container Engine
        Loaded: loaded (/lib/systemd/system/docker.service; enabled; preset: enabled)
        Active: active (running) since Thu 2024-04-11 18:32:27 +0330; 3min 7s ago
   TriggeredBy: ● docker.socket
          Docs: https://docs.docker.com
      Main PID: 25007 (dockerd)
         Tasks: 9
        Memory: 92.4M
           CPU: 2.538s
        CGroup: /system.slice/docker.service
                └─25007 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
   <Snip>
   ```
   
5. Unlock the Swarm to admit the restarted manager. You will only have to perform this step if your Swarm is locked. If you can’t remember your Swarm unlock key, run a `docker swarm unlock-key` command on a different manager.
   ```
   swarm-manager-02:~# docker swarm unlock
      Please enter unlock key: <your key>
   ```  


# Recovering a Swarm  
Restoring a Swarm from backup is only for situations where the swarm is corrupted, or otherwise lost, and you cannot recover objects from copies of config files. You’ll need the `swarm.bkp` and a copy of your swarm’s unlock key (if your swarm is locked).  

The following requirements must be met for a recovery operation to work:  
1. You can only restore to a node running the same version of Docker the backup was performed on  
2. You can only restore to a node with the same IP address as the node the backup was performed on  

Perform the operation on the manager you performed the backup from. You may need to prefix commands with sudo.  

1. Stop Docker on the manager.
   ```
   swarm-manager-02:~# service docker start
   ```
   
2. Delete the Swarm config.
   ```
   swarm-manager-02:~# rm -r /var/lib/docker/swarm
   ```
   At this point, the manager is down and ready for the restore operation.  

3. Restore the Swarm configuration from the backup file and verify the files recover properly.  
   In this example, we’ll restore from a zipped `tar` file called `swarm.bkp`. Restoring to the root directory is required with this command as it will include the full path to the original files as part of the extract operation. This may be different in your environment.
   ```
   swarm-manager-02:~# tar -zxvf swarm.bkp -C /
   ```  

4. Start Docker.
   ```
   swarm-manager-02:~# service docker start
   ```

5. Unlock your Swarm with your Swarm unlock key.
   ```
   swarm-manager-02:~# docker swarm unlock
   Please enter unlock key: <your key>
   ```  
   
6.  Initialize a new swarm with the configuration from the backup. Be sure to use the appropriate IP address for the node you’re performing the restore operation on.  
   ```
   swarm-manager-02:~# docker swarm init --force-new-cluster \
      --advertise-addr 172.16.188.12:2377 \
      --listen-addr 172.16.188.12:2377

   Swarm initialized: current node (jhsg...3l9h) is now a manager.
   ```  

7. Check that the unimatrix01 network was recovered as part of the operation.  
   ```
   swarm-manager-02:~# docker network ls
   NETWORK ID        NAME               DRIVER     SCOPE
   z21s5v82by8q      unimatrix01        overlay    swarm
   ```
   Congratulations. The Swarm is recovered.
   
8. Add new managers and workers and take fresh backups.

Remember to test this procedure regularly and thoroughly. You do not want it to fail when you need it most!  
